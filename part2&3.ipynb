{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning data\n",
    "\n",
    "add column names\n",
    "\n",
    "NOTE : do not run these 3 codes , it would change the csv again but in a wrong way   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7466/2388293107.py:2: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='ISO-8859-1' ,header=None)\n"
     ]
    }
   ],
   "source": [
    "###do not run\n",
    "file_path = 'training.1600000.processed.noemoticon.csv'\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1' ,header=None)\n",
    "\n",
    "column_names = ['label', 'id', 'date', 'query', 'user', 'tweet']  \n",
    "\n",
    "df.columns = column_names\n",
    "\n",
    "df.to_csv('training.1600000.processed.noemoticon.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terminate duplicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###do not run\n",
    "df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "df.to_csv('training.1600000.processed.noemoticon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "turn it to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###do not run\n",
    "df = df.applymap(lambda x: x.lower() if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run this code if you want to insert csv in to a dataframe for checking codes on continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'training.1600000.processed.noemoticon.csv'\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIPELINE:\n",
    "Import Libraries: First, import the necessary libraries for text preprocessing and machine learning.\n",
    "\n",
    "Load the Dataset: Load your dataset containing texts and corresponding labels.\n",
    "\n",
    "Text Preprocessing:\n",
    "\n",
    "Tokenization: Split the text into individual words or tokens.\n",
    "Lowercasing: Convert all words to lowercase to ensure uniformity.\n",
    "Removing Punctuation: Remove any punctuation marks from the text.\n",
    "Removing Stopwords: Remove common words that do not contribute much to the meaning of the text (e.g., \"the\", \"is\", \"and\").\n",
    "Stemming or Lemmatization: Reduce words to their root form to normalize the text. Choose either stemming or lemmatization based on your preference.\n",
    "Feature Extraction: Convert the preprocessed text into numerical feature vectors using techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or Count Vectorization.\n",
    "\n",
    "Model Training: Train a machine learning model on the preprocessed text data. This can be a classifier like Naive Bayes, Logistic Regression, or any other suitable algorithm depending on your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names (words):\n",
      "['00' '000' '06' ... 'â½ã' 'â¾' 'â¾ã']\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "vectorizer_bow = CountVectorizer(max_features=4000)\n",
    "\n",
    "X_bow = vectorizer_bow.fit_transform(df['tweet'])\n",
    "\n",
    "feature_names_bow = vectorizer_bow.get_feature_names_out()\n",
    "\n",
    "print(\"Feature names (words):\")\n",
    "print(feature_names_bow)\n",
    "print(len(feature_names_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "navie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'alpha': 2.0}\n",
      "Validation Accuracy: 0.78\n",
      "Test Accuracy: 0.78\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79     79815\n",
      "           4       0.81      0.74      0.77     80017\n",
      "\n",
      "    accuracy                           0.78    159832\n",
      "   macro avg       0.78      0.78      0.78    159832\n",
      "weighted avg       0.78      0.78      0.78    159832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(df['tweet'], df['label'], test_size=0.2, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "vectorizer_bow = CountVectorizer()\n",
    "\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_valid_bow = vectorizer_bow.transform(X_valid)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0]}\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=nb_classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_bow, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "predictions_valid_bow = best_classifier.predict(X_valid_bow)\n",
    "accuracy_valid_bow = accuracy_score(y_valid, predictions_valid_bow)\n",
    "print(f\"Validation Accuracy: {accuracy_valid_bow:.2f}\")\n",
    "\n",
    "predictions_test_bow = best_classifier.predict(X_test_bow)\n",
    "accuracy_test_bow = accuracy_score(y_test, predictions_test_bow)\n",
    "print(f\"Test Accuracy: {accuracy_test_bow:.2f}\")\n",
    "\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, predictions_test_bow))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "navie classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(max_features=4000)\n",
    "\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "predictions_tfidf = classifier.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_tfidf = accuracy_score(y_test, predictions_tfidf)\n",
    "print(f\"Accuracy: {accuracy_tfidf:.2f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions_tfidf))\n",
    "\n",
    "feature_names_tfidf = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "print(\"Feature names (words):\")\n",
    "print(feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Classifier: {'alpha': 2.0}\n",
      "Validation Accuracy (TF-IDF): 0.77\n",
      "Test Accuracy (TF-IDF): 0.77\n",
      "Classification Report (Test Set - TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77     79815\n",
      "           4       0.78      0.76      0.77     80017\n",
      "\n",
      "    accuracy                           0.77    159832\n",
      "   macro avg       0.77      0.77      0.77    159832\n",
      "weighted avg       0.77      0.77      0.77    159832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(df['tweet'], df['label'], test_size=0.2, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=4000)\n",
    "\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_valid_tfidf = vectorizer_tfidf.transform(X_valid)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0]}\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=nb_classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters for Classifier:\", grid_search.best_params_)\n",
    "\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "predictions_valid_tfidf = best_classifier.predict(X_valid_tfidf)\n",
    "accuracy_valid_tfidf = accuracy_score(y_valid, predictions_valid_tfidf)\n",
    "print(f\"Validation Accuracy (TF-IDF): {accuracy_valid_tfidf:.2f}\")\n",
    "\n",
    "predictions_test_tfidf = best_classifier.predict(X_test_tfidf)\n",
    "accuracy_test_tfidf = accuracy_score(y_test, predictions_test_tfidf)\n",
    "print(f\"Test Accuracy (TF-IDF): {accuracy_test_tfidf:.2f}\")\n",
    "\n",
    "print(\"Classification Report (Test Set - TF-IDF):\")\n",
    "print(classification_report(y_test, predictions_test_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistis classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/usr/lib/python3/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.78825966        nan 0.7895884         nan 0.78969164\n",
      "        nan 0.78959779]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Logistic Regression and Vectorizer: {'C': 1.0, 'penalty': 'l2'}\n",
      "Validation Accuracy (TF-IDF): 0.79\n",
      "Test Accuracy (TF-IDF): 0.79\n",
      "Classification Report (Test Set - TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79     79815\n",
      "           4       0.78      0.80      0.79     80017\n",
      "\n",
      "    accuracy                           0.79    159832\n",
      "   macro avg       0.79      0.79      0.79    159832\n",
      "weighted avg       0.79      0.79      0.79    159832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(df['tweet'], df['label'], test_size=0.2, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=4000)\n",
    "\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_valid_tfidf = vectorizer_tfidf.transform(X_valid)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 0.5, 1.0, 2.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "logreg_classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=logreg_classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters for Logistic Regression and Vectorizer:\", grid_search.best_params_)\n",
    "\n",
    "best_logreg_classifier = grid_search.best_estimator_\n",
    "\n",
    "predictions_valid_tfidf = best_logreg_classifier.predict(X_valid_tfidf)\n",
    "accuracy_valid_tfidf = accuracy_score(y_valid, predictions_valid_tfidf)\n",
    "print(f\"Validation Accuracy (TF-IDF): {accuracy_valid_tfidf:.2f}\")\n",
    "\n",
    "predictions_test_tfidf = best_logreg_classifier.predict(X_test_tfidf)\n",
    "accuracy_test_tfidf = accuracy_score(y_test, predictions_test_tfidf)\n",
    "print(f\"Test Accuracy (TF-IDF): {accuracy_test_tfidf:.2f}\")\n",
    "\n",
    "print(\"Classification Report (Test Set - TF-IDF):\")\n",
    "print(classification_report(y_test, predictions_test_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic for bow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/usr/lib/python3/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.78860534        nan 0.78869058        nan 0.7886812\n",
      "        nan 0.78863975]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Logistic Regression and Bag-of-Words: {'C': 0.5, 'penalty': 'l2'}\n",
      "Validation Accuracy (Bag-of-Words): 0.79\n",
      "Test Accuracy (Bag-of-Words): 0.79\n",
      "Classification Report (Test Set - Bag-of-Words):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78     79815\n",
      "           4       0.78      0.81      0.79     80017\n",
      "\n",
      "    accuracy                           0.79    159832\n",
      "   macro avg       0.79      0.79      0.79    159832\n",
      "weighted avg       0.79      0.79      0.79    159832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(df['tweet'], df['label'], test_size=0.2, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "vectorizer_bow = CountVectorizer(max_features=4000)\n",
    "\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_valid_bow = vectorizer_bow.transform(X_valid)\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 0.5, 1.0, 2.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "logreg_classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=logreg_classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_bow, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters for Logistic Regression and Bag-of-Words:\", grid_search.best_params_)\n",
    "\n",
    "best_logreg_classifier = grid_search.best_estimator_\n",
    "\n",
    "predictions_valid_bow = best_logreg_classifier.predict(X_valid_bow)\n",
    "accuracy_valid_bow = accuracy_score(y_valid, predictions_valid_bow)\n",
    "print(f\"Validation Accuracy (Bag-of-Words): {accuracy_valid_bow:.2f}\")\n",
    "\n",
    "predictions_test_bow = best_logreg_classifier.predict(X_test_bow)\n",
    "accuracy_test_bow = accuracy_score(y_test, predictions_test_bow)\n",
    "print(f\"Test Accuracy (Bag-of-Words): {accuracy_test_bow:.2f}\")\n",
    "\n",
    "print(\"Classification Report (Test Set - Bag-of-Words):\")\n",
    "print(classification_report(y_test, predictions_test_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT:\n",
    "\n",
    "this is the code for fine tuning bert but because of time limit , failed to tuning completely to get the model \n",
    "\n",
    "you would make me so thankfull if consider some percent of grade of this section for this code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "num_classes = 2  \n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "\n",
    "train_input_ids = tokenizer(train_df['tweet'].tolist(), truncation=True, padding=True, return_tensors=\"pt\").input_ids\n",
    "train_labels = torch.tensor(train_df['label'].tolist())\n",
    "\n",
    "val_input_ids = tokenizer(val_df['tweet'].tolist(), truncation=True, padding=True, return_tensors=\"pt\").input_ids\n",
    "val_labels = torch.tensor(val_df['label'].tolist())\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(2):\n",
    "    outputs = model(train_input_ids, labels=train_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(val_input_ids, labels=val_labels)\n",
    "        val_loss = val_outputs.loss\n",
    "\n",
    "    train_preds = torch.argmax(outputs.logits, dim=1)\n",
    "    train_accuracy = (train_preds == train_labels).float().mean()\n",
    "\n",
    "    val_preds = torch.argmax(val_outputs.logits, dim=1)\n",
    "    val_accuracy = (val_preds == val_labels).float().mean()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{2} - '\n",
    "          f'Training Loss: {loss.item():.4f} - Training Accuracy: {train_accuracy.item():.4f} - '\n",
    "          f'Validation Loss: {val_loss.item():.4f} - Validation Accuracy: {val_accuracy.item():.4f}')\n",
    "\n",
    "model.save_pretrained('fine_tuned_bert_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is for hugging face with the same situatition for bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "num_classes = 2  \n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "\n",
    "train_encodings = tokenizer(train_df['tweet'].tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_df['tweet'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_df['label'].tolist())\n",
    "val_dataset = CustomDataset(val_encodings, val_df['label'].tolist())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=2,              \n",
    "    per_device_train_batch_size=8,   \n",
    "    per_device_eval_batch_size=8,    \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset             \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()\n",
    "\n",
    "model.save_pretrained('fine_tuned_bert_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now cause of i dont have bert model , labels in my dataset are via BOW with lack of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these codes are for caculating correlation \n",
    "\n",
    "algorithm:\n",
    "\n",
    "im getting capitalization of each company and gives tweets and their labels that are for those companies , then i would calculate the percent of 4s labels for each company ; for example if tweets for APPLE have labels like this : 4 0 4 4 4 0 4 0 0 4 , then i will add 60% and the apple capitalization to 2 lists and at the end i will calculate the correlation of these 2 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_tweet = 'tweets.csv'\n",
    "df_tweet = pd.read_csv(file_path_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_new = df_tweet['text']\n",
    "\n",
    "X_test_bow_new = vectorizer_bow.transform(X_test_new)\n",
    "\n",
    "predictions_test_bow_new = best_classifier.predict(X_test_bow_new)\n",
    "\n",
    "df_tweet['bow_label'] = predictions_test_bow_new\n",
    "\n",
    "df_tweet.to_csv('tweets.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['BOW_label'] = df_tweet['bow_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('tweets.csv')\n",
    "companies_df = pd.read_csv('companies.csv')\n",
    "entities_df = pd.read_csv('entities.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_entities = entities_df[entities_df['entity_type'] == 'symbol']\n",
    "\n",
    "top_50_strings = entities_df['text'].unique()[:500]\n",
    "\n",
    "symbol_entities_subset = symbol_entities[symbol_entities['text'].isin(top_50_strings)]\n",
    "\n",
    "text_id_mapping = symbol_entities_subset.groupby('text')['tweet_id'].agg(list).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df['ids'] = companies_df['ticker'].map(text_id_mapping.get)\n",
    "good_df = companies_df[companies_df['ids'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2717/625022337.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_df['bow_labels'] = good_df['ids'].apply(map_ids_to_bow_labels)\n"
     ]
    }
   ],
   "source": [
    "def map_ids_to_bow_labels(ids_list):\n",
    "    unique_ids = set(ids_list)\n",
    "    relevant_tweets = tweets_df[tweets_df['id'].isin(unique_ids)]\n",
    "    id_to_bow_labels = relevant_tweets.groupby('id')['bow_label'].apply(list).to_dict()\n",
    "    filtered_ids_list = [id for id in ids_list if id in id_to_bow_labels]\n",
    "    return [id_to_bow_labels.get(id, None) for id in filtered_ids_list]\n",
    "\n",
    "good_df['bow_labels'] = good_df['ids'].apply(map_ids_to_bow_labels)\n",
    "\n",
    "good_df_filtered = good_df[good_df['bow_labels'].apply(lambda x: len(x) > 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_df_filtered['percentage_of_4'] = [lst.count([4]) / len(lst) * 100 if len(lst) > 0 else 0 for lst in good_df_filtered['bow_labels']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = good_df_filtered['percentage_of_4']\n",
    "list2 = good_df_filtered['capitalization']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.10504229152084112\n",
      "Spearman correlation coefficient: 0.14671425621318443\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correlation_pearson = list1.corr(list2, method='pearson')\n",
    "correlation_spearman = list1.corr(list2 , method='spearman')\n",
    "\n",
    "print(\"Pearson correlation coefficient:\", correlation_pearson)\n",
    "print(\"Spearman correlation coefficient:\", correlation_spearman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correlation in next part :\n",
    "\n",
    "algorithm:\n",
    "\n",
    "something like the last part but in 1 monthes , and we can count how many times labels of tweets for each company change , then calculate correlation between all tweets count in 1 month and change counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        [[Fri May 19 06:26:11 +0000 2017], [Fri May 19...\n",
      "39       [[Thu May 18 22:03:25 +0000 2017], [Thu May 18...\n",
      "59       [[Thu May 18 22:50:06 +0000 2017], [Fri May 19...\n",
      "65       [[Thu May 18 22:00:12 +0000 2017], [Thu May 18...\n",
      "182      [[Thu May 18 22:08:20 +0000 2017], [Thu May 18...\n",
      "                               ...                        \n",
      "29691    [[Fri May 19 00:07:40 +0000 2017], [Fri May 19...\n",
      "29707    [[Thu May 18 22:00:22 +0000 2017], [Thu May 18...\n",
      "29917    [[Sat May 20 08:43:36 +0000 2017], [Tue May 23...\n",
      "29937    [[Thu May 18 22:48:05 +0000 2017], [Thu May 18...\n",
      "29988    [[Fri May 19 00:01:25 +0000 2017], [Fri May 19...\n",
      "Name: created_at, Length: 260, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2717/2465366264.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_df['created_at'] = good_df['ids'].apply(map_ids_to_bow_labels)\n"
     ]
    }
   ],
   "source": [
    "def map_ids_to_bow_labels(ids_list):\n",
    "    unique_ids = set(ids_list)\n",
    "    relevant_tweets = tweets_df[tweets_df['id'].isin(unique_ids)]\n",
    "    id_to_bow_labels = relevant_tweets.groupby('id')['created_at'].apply(list).to_dict()\n",
    "    filtered_ids_list = [id for id in ids_list if id in id_to_bow_labels]\n",
    "    return [id_to_bow_labels.get(id, None) for id in filtered_ids_list]\n",
    "\n",
    "good_df['created_at'] = good_df['ids'].apply(map_ids_to_bow_labels)\n",
    "\n",
    "good_df_filtered = good_df[good_df['created_at'].apply(lambda x: len(x) > 0)]\n",
    "print(good_df_filtered['created_at'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        [2017-05-19 06:26:11+00:00, 2017-05-19 11:24:4...\n",
      "39       [2017-05-18 22:03:25+00:00, 2017-05-18 22:49:2...\n",
      "59       [2017-05-18 22:50:06+00:00, 2017-05-19 02:15:0...\n",
      "65       [2017-05-18 22:00:12+00:00, 2017-05-18 22:00:2...\n",
      "182      [2017-05-18 22:08:20+00:00, 2017-05-18 22:13:1...\n",
      "                               ...                        \n",
      "29691    [2017-05-19 00:07:40+00:00, 2017-05-19 00:11:2...\n",
      "29707    [2017-05-18 22:00:22+00:00, 2017-05-18 22:01:1...\n",
      "29917    [2017-05-20 08:43:36+00:00, 2017-05-23 00:05:1...\n",
      "29937    [2017-05-18 22:48:05+00:00, 2017-05-18 23:57:3...\n",
      "29988    [2017-05-19 00:01:25+00:00, 2017-05-19 04:52:0...\n",
      "Name: created_at, Length: 260, dtype: object\n"
     ]
    }
   ],
   "source": [
    "good_df_filtered['created_at'] = good_df_filtered['created_at'].apply(lambda x: [pd.to_datetime(date[0], format='%a %b %d %H:%M:%S %z %Y') for date in x])\n",
    "\n",
    "print(good_df_filtered['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "def count_changes(values):\n",
    "    changes = 0\n",
    "    prev_value = None\n",
    "    for value in values:\n",
    "        for date in value:\n",
    "            if prev_value is not None:\n",
    "                if (prev_value == [0] and date == [4]) or (prev_value == [4] and date == [0]):\n",
    "                    changes += 1\n",
    "            prev_value = date\n",
    "    return changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[396674, 1661279, 953900, 847354, 274922]\n"
     ]
    }
   ],
   "source": [
    "flat_dates = [date for sublist in good_df_filtered['created_at'] for date in sublist]\n",
    "\n",
    "def count_in_month(date_list):\n",
    "    month_counts = {}\n",
    "    for date in date_list:\n",
    "        month = date.month\n",
    "        if month in month_counts:\n",
    "            month_counts[month] += 1\n",
    "        else:\n",
    "            month_counts[month] = 1\n",
    "    return [month_counts.get(m, 0) for m in range(5, 10)]  \n",
    "\n",
    "month_counts = count_in_month(flat_dates)\n",
    "print(month_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list = [item for sublist in good_df_filtered['bow_labels'] for item in sublist]\n",
    "sublist_size = len(combined_list) // 5\n",
    "\n",
    "sublists = [combined_list[i:i+sublist_size] for i in range(0, len(combined_list), sublist_size)]\n",
    "\n",
    "remainder = len(combined_list) % 5\n",
    "for i in range(remainder):\n",
    "    sublists[i].append(combined_list[-(i+1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_changes(sublist):\n",
    "    changes = 0\n",
    "    for i in range(1, len(sublist)):\n",
    "        if (sublist[i-1] == [0] and sublist[i] == [4]) or (sublist[i-1] == [4] and sublist[i] == [0]):\n",
    "            changes += 1\n",
    "    return changes\n",
    "\n",
    "counts = [count_changes(sublists[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.4586346818643381\n",
      "Spearman correlation coefficient: 0.6\n"
     ]
    }
   ],
   "source": [
    "correlation_pearson, _ = pearsonr(month_counts, counts)\n",
    "correlation_spearman, _ = spearmanr(month_counts, counts)\n",
    "\n",
    "print(\"Pearson correlation coefficient:\", correlation_pearson)\n",
    "print(\"Spearman correlation coefficient:\", correlation_spearman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Bag-of-Words (BOW) and TF-IDF (Term Frequency-Inverse Document Frequency) are classic techniques used in text classification. They have their strengths and weaknesses, particularly in terms of where they might make mistakes:\n",
    "\n",
    "Bag-of-Words (BOW):\n",
    "\n",
    "Mistakes in Semantic Understanding: BOW treats each word as independent, ignoring word order and context. This can lead to mistakes in capturing the semantic meaning of the text. For example, it might struggle with negations or sarcasm.\n",
    "Limited Vocabulary: BOW representations often ignore the importance of rare words. If a rare but critical word appears infrequently in the training data, BOW may not assign it enough importance.\n",
    "Sparse Representation: BOW can result in high-dimensional, sparse feature vectors, especially when dealing with large vocabularies. This can lead to overfitting and make it difficult for classifiers to generalize well, especially with limited training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "\n",
    "Overemphasis on Frequent Terms: While TF-IDF addresses some limitations of BOW by weighing terms by their importance, it still heavily relies on term frequency. If certain terms are overly frequent but not necessarily informative, TF-IDF might assign them high weights, potentially leading to misclassification.\n",
    "Neglecting Semantic Similarity: TF-IDF, like BOW, doesn't consider semantic relationships between words. Thus, it might struggle with synonyms or terms related to the training data but not explicitly present.\n",
    "Sensitive to Noise: TF-IDF can be sensitive to noise in the data. If there are mislabeled documents or outliers in the training set, TF-IDF might assign incorrect importance to certain terms, affecting classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both BOW and TF-IDF can perform well in certain scenarios, particularly when dealing with simple text classification tasks or when computational efficiency is crucial. However, for tasks requiring a deeper understanding of language and context, more advanced techniques like word embeddings or contextual embeddings (e.g., Word2Vec, GloVe, BERT) are often preferred as they capture more nuanced semantic relationships between words and phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning:\n",
    "Transfer learning involves leveraging knowledge gained from training a model on one task and applying it to a different but related task. Instead of training a model from scratch, which requires a large amount of labeled data and computational resources, transfer learning allows us to use pre-trained models that have been trained on large datasets for general tasks like image classification, text generation, or language understanding.\n",
    "\n",
    "Advantages of Transfer Learning:\n",
    "Faster Training: Pre-trained models have already learned general patterns from massive datasets, reducing the amount of training time required for the target task.\n",
    "Better Performance with Limited Data: Transfer learning enables the utilization of knowledge from large datasets, even when the target task has limited labeled data.\n",
    "Generalization: Transfer learning helps models generalize better to new tasks or domains, as they have learned robust features from diverse data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning:\n",
    "Fine-tuning is a specific application of transfer learning where a pre-trained model is further trained on a new dataset or task with specific modifications to adapt it to the new task. Instead of retraining the entire model, fine-tuning involves adjusting the parameters of the pre-trained model to better fit the new task.\n",
    "\n",
    "Advantages of Fine-Tuning:\n",
    "Task-Specific Adaptation: Fine-tuning allows the model to adapt to the nuances of the target task by adjusting its parameters while retaining the knowledge learned during pre-training.\n",
    "Improved Performance: Fine-tuning often leads to improved performance on the target task compared to using the pre-trained model without further adaptation.\n",
    "Cost-Effective: Fine-tuning requires less computational resources compared to training a model from scratch since the pre-trained model provides a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning and fine-tuning can be applied in various projects across different domains:\n",
    "\n",
    "Computer Vision: Pre-trained convolutional neural networks (CNNs) like VGG, ResNet, or EfficientNet can be fine-tuned for specific image classification or object detection tasks.\n",
    "\n",
    "Natural Language Processing (NLP): Pre-trained language models like BERT, GPT, or RoBERTa can be fine-tuned for tasks such as sentiment analysis, text classification, or named entity recognition.\n",
    "\n",
    "Healthcare: Transfer learning can be applied to medical imaging tasks, where pre-trained models on natural images (e.g., ImageNet) are fine-tuned on medical image datasets for tasks like disease diagnosis or tumor detection.\n",
    "\n",
    "Recommendation Systems: Transfer learning can be used in recommendation systems by leveraging pre-trained embeddings or models trained on large-scale datasets to improve the performance of personalized recommendation algorithms.\n",
    "\n",
    "Autonomous Driving: Transfer learning can be applied to perception tasks in autonomous driving, where pre-trained models on general driving scenes are fine-tuned on specific environments or driving conditions to enhance performance and safety.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
